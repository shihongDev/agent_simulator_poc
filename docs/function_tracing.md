# Function Tracing

> Learn how to trace function calls and tool usage in your AI agents

Track every function your agent executes - automatically or manually - to understand failures, optimize performance, and export data for analysis.

**Three ways to trace:**

1. **`@track` decorator** - Automatic tracing for your functions
2. **Manual tracing** - For external APIs and tools
3. **LangChain callback** - For LangChain agents

Choose the method that fits your setup. All traces appear in the dashboard with full context.

## Tracing Methods

### 1. **Automatic Tracing with `@track` Decorator**

The simplest way to trace function calls is using the `@track` decorator:

```python  theme={null}
from janus_sdk import track

@track
def analyze_data(data: dict) -> dict:
    """This function call will be automatically traced"""
    # Your analysis logic here
    return {"result": "analysis_complete"}

@track
async def fetch_external_data(url: str) -> str:
    """Async functions are also supported"""
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        return response.text
```

<Info>
  The `@track` decorator automatically captures function inputs, outputs, execution time, and any errors that occur.
</Info>

You can also add rule validation to traced functions:

```python  theme={null}
@track(rules=["Must validate input", "Response time < 100ms"])
def analyze_data_with_rules(data: dict) -> dict:
    """This function call will be automatically traced and validated"""
    # Your analysis logic here
    return {"result": "analysis_complete"}
```

### 2. **Manual Tracing for External Tools**

For external APIs, databases, or tools that can't use the decorator:

```python  theme={null}
from janus_sdk import start_tool_event, finish_tool_event, record_tool_event

# One-shot tracing for simple operations
record_tool_event("external_api", "request_data", "response_data")

# Start/finish pattern for long-running operations
handle = start_tool_event("database_query", "SELECT * FROM users")
try:
    result = query_database()
    finish_tool_event(handle, result)
except Exception as e:
    finish_tool_event(handle, error=e)
```

You can also add rules to manual tracing:

```python  theme={null}
# One-shot tracing with rules
record_tool_event(
    "external_api", 
    "request_data", 
    "response_data",
    rules=["Must use HTTPS", "Response time < 500ms"]
)

# Start/finish pattern with rules
handle = start_tool_event(
    "database_query", 
    "SELECT * FROM users",
    rules=["Must use parameterized queries", "Query timeout < 2s"]
)
try:
    result = query_database()
    finish_tool_event(handle, result)
except Exception as e:
    finish_tool_event(handle, error=e)
```

### 3. **LangChain Integration**

For LangChain agents, use the custom callback:

```python  theme={null}
from janus_sdk import JanusLangChainCallback

agent_executor = AgentExecutor(
    agent=agent,
    tools=[weather_tool, calculator_tool],
    callbacks=[JanusLangChainCallback()]
)
```

## What Gets Traced

### Function Execution

* Function name, signature, and parameters
* Input values and return outputs
* Execution time and performance metrics
* Error messages and stack traces

### Tool & API Calls

* Tool/API name and endpoints
* Request parameters and response data
* Success/failure status
* Duration and timing information

### Agent Coordination

* Sub-agent calls and delegation patterns
* Agent-to-agent communication
* Hierarchical structures
* Context sharing across agents

### Validation & Compliance

* Rule compliance status per function
* Violation details with descriptions
* Custom validation results
* Performance against thresholds

<Info>
  **All trace data** is automatically sent to the Janus dashboard and can be exported for analysis, debugging, or fine-tuning.
</Info>

## Tracing in Practice

### **Healthcare Example**

Here's how tracing works in a healthcare application:

```python  theme={null}
class HealthcareAgent:
    @track
    def analyze_symptom_severity(self, symptoms: list, impact_level: str) -> dict:
        """Automatically traced function"""
        # Analysis logic here
        return severity_assessment

    @track(rules=["Must validate symptoms list", "Must return severity level"])
    def analyze_symptom_severity_with_rules(self, symptoms: list, impact_level: str) -> dict:
        """Automatically traced and validated function"""
        # Analysis logic here
        return severity_assessment

    async def get_medication_info(self, medication_name: str) -> str:
        """Manual tracing with rules for external database"""
        handle = start_tool_event(
            "medication_database", 
            f"query: {medication_name}",
            rules=["Must use secure connection", "Query timeout < 2s"]
        )
        
        try:
            # Simulate database query
            await asyncio.sleep(0.5)
            result = "Medication information retrieved"
            finish_tool_event(handle, result)
            return result
        except Exception as e:
            finish_tool_event(handle, error=e)
            return f"Error: {str(e)}"
```

### **LangChain Integration Example**

For LangChain agents, tracing happens automatically:

```python  theme={null}
class WeatherTool(BaseTool):
    name: str = "search_weather"
    description: str = "Get weather information for a location"
    
    def _run(self, location: str):
        # This tool call will be automatically traced
        return f"Weather for {location}: Sunny, 75Â°F"

# The callback automatically traces all tool calls
callback = JanusLangChainCallback()
agent_executor = AgentExecutor(
    agent=agent,
    tools=[WeatherTool()],
    callbacks=[callback]
)
```

### **Advanced LangChain Integration**

For more control over tool tracing, you can create a custom callback handler:

```python  theme={null}
from langchain.callbacks.base import BaseCallbackHandler
from janus_sdk import start_tool_event, finish_tool_event

class JanusLangChainCallback(BaseCallbackHandler):
    def __init__(self):
        self._handles = {}
    
    def on_agent_action(self, action, **kwargs):
        if hasattr(action, 'tool') and action.tool:
            self._handles[action.tool] = start_tool_event(
                tool_name=action.tool,
                tool_input=action.tool_input
            )
    
    def on_agent_finish(self, outputs, **kwargs):
        for tool, handle in self._handles.items():
            finish_tool_event(handle, tool_output=str(outputs))
        self._handles.clear()

# Add to your agent executor
executor = AgentExecutor(
    agent=agent, 
    tools=[your_tools], 
    callbacks=[JanusLangChainCallback()]
)
```

This custom implementation gives you fine-grained control over when tool events start and finish, allowing for more precise tracing of complex agent workflows.

## Rule Validation

### **Adding Rules to Traces**

You can add rule validation to any traced function or tool call by passing a list of rule strings:

```python  theme={null}
# Function-level rules
@track(rules=["Must validate input", "Response time < 100ms"])
def my_function(data: dict) -> dict:
    pass

# Tool-level rules
handle = start_tool_event(
    "database_query", 
    "SELECT * FROM users",
    rules=["Must use parameterized queries", "Query timeout < 2s"]
)

# One-shot tool rules
record_tool_event(
    "external_api", 
    "request_data", 
    "response_data",
    rules=["Must use HTTPS", "Response time < 500ms"]
)
```

### **Rule Format**

Rules are simple strings that describe the expected behavior:

* **Input validation**: "Must validate input before processing"
* **Performance**: "Response time must be under 100ms"
* **Security**: "Must use HTTPS for external calls"
* **Format**: "Output must be valid JSON"
* **Business logic**: "Must check user permissions"

## Viewing Traces

### **In the Janus Dashboard**

Traces are automatically recorded, evaluated, and sent to the Janus platform when simulations are run. You can view them in the dashboard:

1. **Go to Results** at [app.withjanus.com](https://app.withjanus.com)
2. **Select a simulation** to view detailed results
3. **Click on the Traces tab** to see:
   * Function call hierarchy
   * Input/output values
   * Execution timing
   * Error details
   * Performance metrics

### **Trace Information Available**

* **Function calls**: Complete call stack with parameters
* **Tool usage**: External API calls, database queries, etc.
* **Sub agent calls**: Agent delegation, communication flows, and hierarchical structures
* **Performance data**: Response times, token usage, throughput
* **Error tracking**: Exceptions, failures, and debugging info
* **Context linking**: Connection to conversation and simulation data

## Best Practices

<Tip>
  * Use `@track` for internal functions that you control
  * Use manual tracing for external APIs and third-party tools
  * Include meaningful tool names and input descriptions
  * Handle errors gracefully in traced functions
  * Use specific, measurable rules rather than vague guidelines
  * Combine rule-based evaluation with autonomous judging for comprehensive assessment
</Tip>

<Warning>
  Tracing adds some overhead to function calls. For high-frequency operations, consider selective tracing.
</Warning>
